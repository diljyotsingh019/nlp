{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Stemming-Lemmatization.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9KkNTZegWcr",
        "colab_type": "text"
      },
      "source": [
        "# Stemming\n",
        "\n",
        "Often when searching text for a certain keyword, it helps if the search returns variations of the word. For instance, searching for \"boat\" might also return \"boats\" and \"boating\". Here, \"boat\" would be the stem for [boat, boater, boating, boats].\n",
        "Stemming is a somewhat crude method for cataloging related words; it essentially chops off letters from the end until the stem is reached. This works fairly well in most cases, but unfortunately English has many exceptions where a more sophisticated process is required.\n",
        "\n",
        "\n",
        "Stemming is a kind of normalization for words. Normalization is a technique where a set of words in a sentence are converted into a sequence to shorten its lookup. The words which have the same meaning but have some variation according to the context or sentence are normalized.\n",
        "\n",
        "In another word, there is one root word, but there are many variations of the same words. \n",
        "\n",
        "For example, the root word is \"run\" and it's variations are \"runs, running, ran\" and like so. \n",
        "\n",
        "In the same way, with the help of Stemming, we can find the root word of any variations.\n",
        "\n",
        "\n",
        "Stemming is a data-preprocessing module. The English language has many variations of a single word. These variations create ambiguity in machine learning training and prediction. To create a successful model, it's vital to filter such words and convert to the same type of sequenced data using stemming. Also, this is an important technique to get row data from a set of sentence and removal of redundant data also known as normalization.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-poRRDleXrc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Install NLTK if not already installed...uncomment the next cell and run it.\n",
        "#! pip install nltk"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HMPz1FMrfFXA",
        "colab_type": "code",
        "outputId": "950443a1-4c4f-455e-d9d4-67d0552d712e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RDyF9AP_1KJw",
        "colab_type": "text"
      },
      "source": [
        "## Porter Stemmer\n",
        "\n",
        "One of the most common - and effective - stemming tools is Porter's Algorithm developed by Martin Porter in 1980. The algorithm employs five phases of word reduction, each with its own set of mapping rules.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GuK2DqVu1I3W",
        "colab_type": "code",
        "outputId": "d6bb40e4-1459-45e0-f348-b77cddffa5f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "list_of_words= [\"wait\", \"waiting\", \"waited\", \"waits\", 'run','runner','running','ran','runs','easily','fairly']\n",
        "ps =PorterStemmer()\n",
        "for w in list_of_words:\n",
        "    rootWord=ps.stem(w)\n",
        "    print(rootWord)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "wait\n",
            "wait\n",
            "wait\n",
            "wait\n",
            "run\n",
            "runner\n",
            "run\n",
            "ran\n",
            "run\n",
            "easili\n",
            "fairli\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lp2W426LBIbb",
        "colab_type": "text"
      },
      "source": [
        "## Snowball Stemmer\n",
        "\n",
        "This is somewhat of a misnomer, as Snowball is the name of a stemming language developed by Martin Porter. The algorithm used here is more acurately called the \"English Stemmer\" or \"Porter2 Stemmer\". It offers a slight improvement over the original Porter stemmer, both in logic and speed. Since nltk uses the name SnowballStemmer, we'll use it here.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mnnVGoc5BKhk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "d1298131-32ca-46a7-b84e-e44184b9b9de"
      },
      "source": [
        "from nltk.stem.snowball import SnowballStemmer\n",
        "\n",
        "# The Snowball Stemmer requires that you pass a language parameter\n",
        "s_stemmer = SnowballStemmer(language='english')\n",
        "words = ['run','runner','running','ran','runs','easily','fairly']\n",
        "\n",
        "for word in words:\n",
        "    print(word+' --> '+s_stemmer.stem(word))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "run --> run\n",
            "runner --> runner\n",
            "running --> run\n",
            "ran --> ran\n",
            "runs --> run\n",
            "easily --> easili\n",
            "fairly --> fair\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Pld58XG194q",
        "colab_type": "text"
      },
      "source": [
        "## Lemmatization\n",
        "\n",
        "Lemmatization is the process of finding the lemma of a word depending on their meaning. \n",
        "\n",
        "Lemmatization usually refers to the morphological analysis of words, which aims to remove inflectional endings. \n",
        "It helps in returning the base or dictionary form of a word, which is known as the lemma. \n",
        "\n",
        "The NLTK Lemmatization method is based on WorldNet's built-in morph function. \n",
        "Text preprocessing includes both stemming as well as lemmatization. \n",
        "Many people find the two terms confusing. Some treat these as same, but there is a difference between these both. \n",
        "\n",
        "Lemmatization is preferred over Stemming because of the below reason.\n",
        "\n",
        "Why is Lemmatization better than Stemming?\n",
        "\n",
        "Stemming algorithm works by cutting the suffix from the word. In a broader sense cuts either the beginning or end of the word.\n",
        "\n",
        "On the contrary, Lemmatization is a more powerful operation, and it takes into consideration morphological analysis of the words. It returns the lemma which is the base form of all its inflectional forms. \n",
        "In-depth linguistic knowledge is required to create dictionaries and look for the proper form of the word. \n",
        "\n",
        "Stemming is a general operation while lemmatization is an intelligent operation where the proper form will be looked in the dictionary. Hence, lemmatization helps in forming better machine learning features.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m_Y73zpF3QFL",
        "colab_type": "code",
        "outputId": "eccfaf85-e917-4451-9edb-dbac43ebff2a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "nltk.download('wordnet')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "exYNkctx2iZq",
        "colab_type": "code",
        "outputId": "f1c94762-56a9-4e6c-ec2a-c28ac4ea5792",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        }
      },
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "text = \"buying buys studies studying cries cry\"\n",
        "tokenization = nltk.word_tokenize(text)\n",
        "\n",
        "for w in tokenization:\n",
        " print(\"Lemma for {} is {}\".format(w, wordnet_lemmatizer.lemmatize(w)))  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Lemma for buying is buying\n",
            "Lemma for buys is buy\n",
            "Lemma for studies is study\n",
            "Lemma for studying is studying\n",
            "Lemma for cries is cry\n",
            "Lemma for cry is cry\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L8NwN0dJ4FDK",
        "colab_type": "text"
      },
      "source": [
        "## Difference between Stemming and Lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5oVQ7E-G327u",
        "colab_type": "code",
        "outputId": "b269bcaa-a835-4ec1-da47-65ad5222bbd0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        }
      },
      "source": [
        "\n",
        "porter_stemmer  = PorterStemmer()\n",
        "\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "text = \"studies studying cries cry\"\n",
        "tokenization = nltk.word_tokenize(text)\n",
        "for w in tokenization:\n",
        " print(\"\\n Stemming for {} is {}\".format(w,porter_stemmer.stem(w))) \n",
        " print(\" Lemma for {} is {} \\n\".format(w, wordnet_lemmatizer.lemmatize(w)))  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Stemming for studies is studi\n",
            " Lemma for studies is study \n",
            "\n",
            "\n",
            " Stemming for studying is studi\n",
            " Lemma for studying is studying \n",
            "\n",
            "\n",
            " Stemming for cries is cri\n",
            " Lemma for cries is cry \n",
            "\n",
            "\n",
            " Stemming for cry is cri\n",
            " Lemma for cry is cry \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}