{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yPby3kogLVzM"
   },
   "source": [
    "# Bag of Words (BoW)\n",
    "\n",
    "Bag of Words is a method for feature generation from textual documents.\n",
    "\n",
    "In the Text Classification problem, we have a set of texts and their respective labels. But we directly can't use text for our model. We need to convert these text into some numbers or vectors of numbers.\n",
    "\n",
    "Bag-of-words model(BoW ) is the simplest way of extracting features from the text. \n",
    "BoW converts text into the matrix of occurrence of words within a document. \n",
    "This model concerns about whether given words occurred/appeared or not in the document. We can create a matrix of document and words by counting the occurrence of words in the given document. This matrix is known as Document-Term Matrix(DTM).\n",
    "\n",
    "This matrix is using a single word. It can be a combination of two or more words, which is called a bigram or trigram model and the general approach is called the n-gram model.\n",
    "\n",
    "You can generate document term matrix by using scikit-learn's CountVectorizer.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7c_MWKNIYLaG"
   },
   "source": [
    "### Defining Bag of Words (BoW) Vectors\n",
    "\n",
    "Bag of Words (BoW) refers to the representation of text which describes the presence of words within the text data. \n",
    "\n",
    "The intuition behind this is that two similar text fields will contain similar kind of words, and will therefore have a similar bag of words. \n",
    "\n",
    "Further, that from the text alone we can learn something about the meaning of the document.\n",
    "\n",
    "\n",
    "\n",
    "Let us consider two strings/documents:\n",
    "\n",
    "\n",
    "1: “i am happy to see you”\n",
    "\n",
    "2: “the happy prince looked around”\n",
    "\n",
    "\n",
    "The list of unique words/vocabulary are the following:\n",
    "\n",
    "am, around, happy, i, looked, prince,  see, you, the, to \n",
    "\n",
    "A binary feature vector can be represent each of the documents as follows:\n",
    "\n",
    " am around happy i looked prince see you the to \n",
    "        \n",
    "\n",
    "\n",
    "1:      1           0         1          1         0         0        1       1       0     1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "2:      0           1         1          0         1         1        0       0       1     0\n",
    "\n",
    "\n",
    "Here 1 represents that the word is present in the string/document and 0 means the word is absent.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Similarly is a word is present more than once in a document, the 1 can be replaced with the actual count of the word in the document.\n",
    "\n",
    "Example:\n",
    "\n",
    "If we have a third string/document\n",
    "\n",
    "3: “the happy prince looked around to see you happy”\n",
    "\n",
    "\n",
    " am around happy i looked prince see you the to \n",
    "\n",
    "\n",
    "\n",
    "3:      0   1   2   1   1   1   1   1   1    1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "It is evident that the dimension of this feature vector will be equal to the number of unique words in the text (strings or documents).\n",
    "\n",
    "For a very large document there are chances that many words present in one document will not be present in the other.\n",
    "Therefore, the feature vectors for the documents will hold the values 0’ sin most of the places. \n",
    "\n",
    "To overcome this problem, normally the BoW vectors are represented as sparse vectors, reducing the dimensions of the vector representations of the documents.\n",
    "\n",
    "We generally perform stop-word-removal before finding BoW.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z9Yv4duTMeIG"
   },
   "source": [
    "### Loading Data\n",
    "\n",
    "Now, you will learn Text Classification. We will perform Multi-Nomial Naive Bayes Classification using scikit-learn.\n",
    "\n",
    "In the model the building part, you can use the \"Sentiment Analysis of Movie, Reviews\" dataset available on Kaggle. The dataset is a tab-separated file. Dataset has four columns PhraseId, SentenceId, Phrase, and Sentiment.\n",
    "\n",
    "This data has 5 sentiment labels:\n",
    "\n",
    "0 - negative 1 - somewhat negative 2 - neutral 3 - somewhat positive 4 - positive\n",
    "\n",
    "Here, you can build a model to classify the type of cultivar. The dataset is available on Kaggle. You can download it from the following link: https://www.kaggle.com/c/sentiment-analysis-on-movie-reviews/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dliZiz-9O01h"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AKQVTebPN68d"
   },
   "outputs": [],
   "source": [
    "\n",
    "data=pd.read_csv('movie-train.tsv', sep='\\t')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tSg7kW2KieFH"
   },
   "source": [
    "## Attributes/Datatypes/Number-of-Entries in the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JUeC1FlSOmGG"
   },
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A_3sbhlvjMhZ"
   },
   "source": [
    "## Look into the First Ten Observations in the DataFrame\n",
    "\n",
    "\"Phrase\" column contains the actual movie reviews.\n",
    "\n",
    "\"Sentiment\" column contains the reviews - labels/categories for the corresponding movie.\n",
    "\n",
    "The sentiment labels are as follows:\n",
    "\n",
    "0 - negative \n",
    "1 - somewhat negative \n",
    "2 - neutral \n",
    "3 - somewhat positive \n",
    "4 - positive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "YiV3BMeXi2sU",
    "outputId": "2d85b595-14d0-4b98-e1f8-214573588402"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              Phrase  Sentiment\n",
      "0  A series of escapades demonstrating the adage ...          1\n",
      "1  A series of escapades demonstrating the adage ...          2\n",
      "2                                           A series          2\n",
      "3                                                  A          2\n",
      "4                                             series          2\n",
      "5  of escapades demonstrating the adage that what...          2\n",
      "6                                                 of          2\n",
      "7  escapades demonstrating the adage that what is...          2\n",
      "8                                          escapades          2\n",
      "9  demonstrating the adage that what is good for ...          2\n"
     ]
    }
   ],
   "source": [
    "print(data[['Phrase','Sentiment']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4Vh1TE9iicMY"
   },
   "source": [
    "## Count of Different Sentiment Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B1KQ0FSrOjhv"
   },
   "outputs": [],
   "source": [
    "data.Sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BeHQaxJ2lmWM"
   },
   "source": [
    "## Plot the Sentiment Categories in the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Tzf2EPxJOs7J"
   },
   "outputs": [],
   "source": [
    "Sentiment_count=data.groupby('Sentiment').count()\n",
    "plt.bar(Sentiment_count.index.values, Sentiment_count['Phrase'])\n",
    "plt.xlabel('Review Sentiments')\n",
    "plt.ylabel('Number of Review')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MGJI-MdHRZMK"
   },
   "source": [
    "## Find BoW Feature Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FR17-FREPJE_"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "# Tokenizer to remove unwanted elements from out data like symbols and numbers\n",
    "token = RegexpTokenizer(r'[a-zA-Z0-9]+')\n",
    "\n",
    "cv = CountVectorizer(lowercase=True,stop_words='english',ngram_range = (1,1),tokenizer = token.tokenize)\n",
    "text_counts= cv.fit_transform(data['Phrase'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 884
    },
    "colab_type": "code",
    "id": "dfSQCNxslHaa",
    "outputId": "565ac806-110d-430a-db4d-0569ac212636"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 11671)\t1\n",
      "  (0, 4517)\t1\n",
      "  (0, 3444)\t1\n",
      "  (0, 294)\t1\n",
      "  (0, 5735)\t2\n",
      "  (0, 5751)\t1\n",
      "  (0, 5512)\t1\n",
      "  (0, 9065)\t1\n",
      "  (0, 593)\t1\n",
      "  (0, 584)\t1\n",
      "  (0, 12673)\t1\n",
      "  (1, 11671)\t1\n",
      "  (1, 4517)\t1\n",
      "  (1, 3444)\t1\n",
      "  (1, 294)\t1\n",
      "  (1, 5735)\t1\n",
      "  (1, 5751)\t1\n",
      "  (2, 11671)\t1\n",
      "  (4, 11671)\t1\n",
      "  (5, 4517)\t1\n",
      "  (5, 3444)\t1\n",
      "  (5, 294)\t1\n",
      "  (5, 5735)\t1\n",
      "  (5, 5751)\t1\n",
      "  (7, 4517)\t1\n",
      "  :\t:\n",
      "  (156050, 11305)\t1\n",
      "  (156050, 9054)\t1\n",
      "  (156051, 11305)\t1\n",
      "  (156051, 9054)\t1\n",
      "  (156052, 11305)\t1\n",
      "  (156053, 11281)\t1\n",
      "  (156053, 1281)\t1\n",
      "  (156053, 5252)\t1\n",
      "  (156053, 6156)\t1\n",
      "  (156053, 1006)\t1\n",
      "  (156053, 2271)\t1\n",
      "  (156054, 11281)\t1\n",
      "  (156054, 5252)\t1\n",
      "  (156054, 6156)\t1\n",
      "  (156054, 1006)\t1\n",
      "  (156054, 2271)\t1\n",
      "  (156055, 11281)\t1\n",
      "  (156055, 6156)\t1\n",
      "  (156056, 5252)\t1\n",
      "  (156056, 1006)\t1\n",
      "  (156056, 2271)\t1\n",
      "  (156057, 1006)\t1\n",
      "  (156057, 2271)\t1\n",
      "  (156058, 1006)\t1\n",
      "  (156059, 2271)\t1\n"
     ]
    }
   ],
   "source": [
    "print(text_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KEgRbWl7RLL9"
   },
   "source": [
    "## Split Data into Training and Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mGjoaWVMMI2_"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(text_counts, data['Sentiment'], test_size=0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cZfaG4NFRJ6X"
   },
   "source": [
    "## Classification with Naive Bayes Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iXDYqT4TMPmS"
   },
   "outputs": [],
   "source": [
    "# Import the class for MultinomialNaiveBayes from sklearn library\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BNVb4zgOjvLF"
   },
   "source": [
    "### Train the Model on Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZOQ5gEMXjxeu"
   },
   "outputs": [],
   "source": [
    "# Model Generation Using Multinomial Naive Bayes\n",
    "clf = MultinomialNB().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FQsA0aeJjz9b"
   },
   "source": [
    "### Evaluate the Model on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "-9S7I8XAjy5D",
    "outputId": "e6486e09-1b49-4e7b-eccb-0129d02d5406"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB Accuracy: 0.6049169122986885\n"
     ]
    }
   ],
   "source": [
    "predicted= clf.predict(X_test)\n",
    "print(\"MultinomialNB Accuracy:\",metrics.accuracy_score(y_test, predicted))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "BoW.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
