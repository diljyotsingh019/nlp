{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s9KkNTZegWcr"
   },
   "source": [
    "# Text Tokenization\n",
    "\n",
    "Tokenization is the process by which a big quantity of text is divided into smaller parts called tokens.\n",
    "\n",
    "A paragraph is composed of sentences. Each sentence, in turn is composed of many words.\n",
    "\n",
    "The segmentation can be done on a given paragraph of text at the level of sentences (tokenizing sentences) and each sentence can further be tokenized into many words it is composed of (word tokenization).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5-poRRDleXrc"
   },
   "outputs": [],
   "source": [
    "# Install NLTK if not already installed...uncomment the next cell and run it.\n",
    "#! pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "HMPz1FMrfFXA",
    "outputId": "b3a2df58-6e27-4409-e439-a72f0870c3d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4uviosf-fQN8"
   },
   "source": [
    "## Tokenize Words\n",
    "\n",
    "From a given text, we can extract out individual words by word tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Fps2E0kB7Ekx",
    "outputId": "dd82eb51-88d6-4dd5-b556-0a369c33e4ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dogs', '.']\n"
     ]
    }
   ],
   "source": [
    "# Word Tokenization\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"A quick brown fox jumps over the lazy dogs.\"\n",
    "list_of_words = word_tokenize(text)\n",
    "print(list_of_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fYQN9_oj89UH"
   },
   "source": [
    "## Split Method\n",
    "\n",
    "Split method can be used to extract out individual words from a string.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "V8YL9XNyesxa",
    "outputId": "4a6e0af8-8d91-40c2-cdb7-e8720255c0ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Split: ['learn', 'python', 'from', 'me', 'and', 'make', 'your', 'life', 'easy']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "text =\"learn python from me and make your life easy\".split()\n",
    "\n",
    "print(\"After Split:\",text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PsdsQWS3fMwm"
   },
   "source": [
    "## Tokenize Sentence\n",
    "\n",
    "Sentence tokenizer extracts individual sentences from a given text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "dKv5qsisfL58",
    "outputId": "8ca7ebb4-16aa-4b06-ff6c-a1ecff9a3693"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Beauty lies in the eyes of the beholder.', 'Do not open your eyes.', 'A thing of beauty is a joy forever.']\n"
     ]
    }
   ],
   "source": [
    "# Sentence Tokenization\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "text = \"Beauty lies in the eyes of the beholder. Do not open your eyes. A thing of beauty is a joy forever.\"\n",
    "sentences = sent_tokenize(text)\n",
    "print(sentences)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GjoRKxu3Wf14"
   },
   "source": [
    "### Tokenize Text Read from a Disk File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "hMe-20UGinNL",
    "outputId": "a3fab611-5265-4344-9620-cc166ba7debe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a NLTK test file.\n",
      "How are you India?\n",
      "Let's go green.\n",
      "Playing in the garden.\n",
      "Roaming in the breeze.\n",
      "Why is the child crying?\n"
     ]
    }
   ],
   "source": [
    "# Reading local files from disk\n",
    "f = open('Test.txt')\n",
    "raw = f.read()\n",
    "print(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "o4kZcVvSiqV1",
    "outputId": "920b8dce-ba3a-4e8b-e044-e8547c787ee4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This is a NLTK test file.', 'How are you India?', \"Let's go green.\", 'Playing in the garden.', 'Roaming in the breeze.', 'Why is the child crying?']\n"
     ]
    }
   ],
   "source": [
    "# Tokenize sentences\n",
    "sentences = sent_tokenize(raw)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "es7UyZsjjBSu",
    "outputId": "82a42c17-81a9-4691-bea5-7c664e722ef9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'a', 'NLTK', 'test', 'file', '.']\n",
      "['How', 'are', 'you', 'India', '?']\n",
      "['Let', \"'s\", 'go', 'green', '.']\n",
      "['Playing', 'in', 'the', 'garden', '.']\n",
      "['Roaming', 'in', 'the', 'breeze', '.']\n",
      "['Why', 'is', 'the', 'child', 'crying', '?']\n"
     ]
    }
   ],
   "source": [
    "# Tokenize words in the a sentence\n",
    "for sentence in sentences:\n",
    "    list_of_words = word_tokenize(sentence)\n",
    "    print(list_of_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "FvWq992WishB",
    "outputId": "aaf3fad9-4d07-42bd-c1d9-0c1199e992f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a NLTK test file.\n",
      "How are you India?\n",
      "Let's go green.\n",
      "Playing in the garden.\n",
      "Roaming in the breeze.\n",
      "Why is the child crying?\n"
     ]
    }
   ],
   "source": [
    "f = open('Test.txt','r')\n",
    "for line in f:\n",
    " print(line.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "09smJM61ix8U"
   },
   "source": [
    "## Tokenize a String Read from Keyboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "3Xlz1xaiixLt",
    "outputId": "280680da-e650-460b-86c5-a2e0532ade31"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter some text: If you miss the train I am on, you will know that I am gone. \n",
      "You typed 17 words.\n"
     ]
    }
   ],
   "source": [
    "s = input(\"Enter some text: \")\n",
    "print (\"You typed\", len(nltk.word_tokenize(s)), \"words.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "WqNRDtxMj5Wf",
    "outputId": "58a41f4d-fb89-49d0-b2e6-dcd9089fa70a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['If', 'you', 'miss', 'the', 'train', 'I', 'am', 'on', ',', 'you', 'will', 'know', 'that', 'I', 'am', 'gone', '.']\n"
     ]
    }
   ],
   "source": [
    "list_of_words = word_tokenize(s)\n",
    "print(list_of_words)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Text-Tokenization.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
