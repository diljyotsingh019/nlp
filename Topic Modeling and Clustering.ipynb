{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-24T12:42:59.941160Z",
     "start_time": "2022-04-24T12:42:57.511921Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\diljyot\\anaconda3\\lib\\site-packages (4.1.2)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\diljyot\\anaconda3\\lib\\site-packages (from gensim) (5.2.1)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\diljyot\\anaconda3\\lib\\site-packages (from gensim) (1.21.6)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\diljyot\\anaconda3\\lib\\site-packages (from gensim) (1.6.2)\n",
      "Requirement already satisfied: Cython==0.29.23 in c:\\users\\diljyot\\anaconda3\\lib\\site-packages (from gensim) (0.29.23)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-24T12:43:02.103065Z",
     "start_time": "2022-04-24T12:42:59.944173Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imblearn in c:\\users\\diljyot\\anaconda3\\lib\\site-packages (0.0)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: imbalanced-learn in c:\\users\\diljyot\\anaconda3\\lib\\site-packages (from imblearn) (0.9.0)\n",
      "Requirement already satisfied: numpy>=1.14.6 in c:\\users\\diljyot\\anaconda3\\lib\\site-packages (from imbalanced-learn->imblearn) (1.21.6)\n",
      "Requirement already satisfied: scipy>=1.1.0 in c:\\users\\diljyot\\anaconda3\\lib\\site-packages (from imbalanced-learn->imblearn) (1.6.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\diljyot\\anaconda3\\lib\\site-packages (from imbalanced-learn->imblearn) (2.2.0)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\diljyot\\anaconda3\\lib\\site-packages (from imbalanced-learn->imblearn) (1.0.1)\n",
      "Requirement already satisfied: scikit-learn>=1.0.1 in c:\\users\\diljyot\\anaconda3\\lib\\site-packages (from imbalanced-learn->imblearn) (1.0.2)\n"
     ]
    }
   ],
   "source": [
    "pip install imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-24T12:43:04.240908Z",
     "start_time": "2022-04-24T12:43:02.107052Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyLDAvis in c:\\users\\diljyot\\anaconda3\\lib\\site-packages (3.3.1)\n",
      "Requirement already satisfied: future in c:\\users\\diljyot\\anaconda3\\lib\\site-packages (from pyLDAvis) (0.18.2)\n",
      "Requirement already satisfied: numpy>=1.20.0 in c:\\users\\diljyot\\anaconda3\\lib\\site-packages (from pyLDAvis) (1.21.6)\n",
      "Requirement already satisfied: pandas>=1.2.0 in c:\\users\\diljyot\\anaconda3\\lib\\site-packages (from pyLDAvis) (1.3.5)\n",
      "Requirement already satisfied: numexpr in c:\\users\\diljyot\\anaconda3\\lib\\site-packages (from pyLDAvis) (2.8.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\diljyot\\anaconda3\\lib\\site-packages (from pyLDAvis) (1.0.1)\n",
      "Requirement already satisfied: funcy in c:\\users\\diljyot\\anaconda3\\lib\\site-packages (from pyLDAvis) (1.17)\n",
      "Requirement already satisfied: sklearn in c:\\users\\diljyot\\anaconda3\\lib\\site-packages (from pyLDAvis) (0.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\diljyot\\anaconda3\\lib\\site-packages (from pyLDAvis) (62.1.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\diljyot\\anaconda3\\lib\\site-packages (from pyLDAvis) (3.0.3)\n",
      "Requirement already satisfied: gensim in c:\\users\\diljyot\\anaconda3\\lib\\site-packages (from pyLDAvis) (4.1.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\diljyot\\anaconda3\\lib\\site-packages (from pyLDAvis) (1.6.2)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\diljyot\\anaconda3\\lib\\site-packages (from pyLDAvis) (1.0.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\diljyot\\anaconda3\\lib\\site-packages (from pandas>=1.2.0->pyLDAvis) (2020.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\diljyot\\anaconda3\\lib\\site-packages (from pandas>=1.2.0->pyLDAvis) (2.8.1)\n",
      "Requirement already satisfied: Cython==0.29.23 in c:\\users\\diljyot\\anaconda3\\lib\\site-packages (from gensim->pyLDAvis) (0.29.23)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\diljyot\\anaconda3\\lib\\site-packages (from gensim->pyLDAvis) (5.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\diljyot\\anaconda3\\lib\\site-packages (from jinja2->pyLDAvis) (2.0.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\diljyot\\anaconda3\\lib\\site-packages (from numexpr->pyLDAvis) (20.4)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\diljyot\\anaconda3\\lib\\site-packages (from scikit-learn->pyLDAvis) (2.2.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\diljyot\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7.3->pandas>=1.2.0->pyLDAvis) (1.15.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\diljyot\\anaconda3\\lib\\site-packages (from packaging->numexpr->pyLDAvis) (2.4.7)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-24T12:43:10.686424Z",
     "start_time": "2022-04-24T12:43:04.243894Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\DILJYOT\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\DILJYOT\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "C:\\Users\\DILJYOT\\anaconda3\\lib\\site-packages\\seaborn\\rcmod.py:82: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(mpl.__version__) >= \"3.0\":\n",
      "C:\\Users\\DILJYOT\\anaconda3\\lib\\site-packages\\setuptools\\_distutils\\version.py:351: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  other = LooseVersion(other)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "from tqdm import tqdm\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, OrdinalEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix , classification_report, accuracy_score, roc_auc_score, plot_roc_curve\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import pickle\n",
    "import spacy\n",
    "import ast\n",
    "\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "pd.set_option('display.max_rows', 800)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-24T12:43:10.885915Z",
     "start_time": "2022-04-24T12:43:10.689414Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tech</td>\n",
       "      <td>tv future in the hands of viewers with home th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>business</td>\n",
       "      <td>worldcom boss  left books alone  former worldc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sport</td>\n",
       "      <td>tigers wary of farrell  gamble  leicester say ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sport</td>\n",
       "      <td>yeading face newcastle in fa cup premiership s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>entertainment</td>\n",
       "      <td>ocean s twelve raids box office ocean s twelve...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        category                                               text\n",
       "0           tech  tv future in the hands of viewers with home th...\n",
       "1       business  worldcom boss  left books alone  former worldc...\n",
       "2          sport  tigers wary of farrell  gamble  leicester say ...\n",
       "3          sport  yeading face newcastle in fa cup premiership s...\n",
       "4  entertainment  ocean s twelve raids box office ocean s twelve..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"bbc-text.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-24T12:43:10.901880Z",
     "start_time": "2022-04-24T12:43:10.887883Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sport            511\n",
       "business         510\n",
       "politics         417\n",
       "tech             401\n",
       "entertainment    386\n",
       "Name: category, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.category.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-24T12:43:10.916841Z",
     "start_time": "2022-04-24T12:43:10.903839Z"
    }
   },
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "file = open(\"stopwords/extended_stopwords.txt\", \"r\")\n",
    "stop_words.extend([line.strip() for line in file.readlines()])\n",
    "file.close()\n",
    "file = open(\"stopwords/contractions.txt\", \"r\")\n",
    "contractions = ast.literal_eval(file.read())\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-24T12:43:10.931767Z",
     "start_time": "2022-04-24T12:43:10.922791Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tech' 'business' 'sport' 'entertainment' 'politics']\n"
     ]
    }
   ],
   "source": [
    "print(df.category.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-24T12:43:10.947723Z",
     "start_time": "2022-04-24T12:43:10.935756Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2225\n"
     ]
    }
   ],
   "source": [
    "# Convert to list\n",
    "data = df.text.values.tolist()\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-24T12:43:11.526210Z",
     "start_time": "2022-04-24T12:43:10.952710Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('tv future in the hands of viewers with home theatre systems plasma '\n",
      " 'high-definition tvs and digital video recorders moving into the living room '\n",
      " 'the way people watch tv will be radically different in five years time. that '\n",
      " 'is according to an expert panel which gathered at the annual consumer '\n",
      " 'electronics show in las vegas to discuss how these new technologies will '\n",
      " 'impact one of our favourite pastimes. with the us leading the trend '\n",
      " 'programmes and other content will be delivered to viewers via home networks '\n",
      " 'through cable satellite telecoms companies and broadband service providers '\n",
      " 'to front rooms and portable devices. one of the most talked-about '\n",
      " 'technologies of ces has been digital and personal video recorders (dvr and '\n",
      " 'pvr). these set-top boxes like the us s tivo and the uk s sky+ system allow '\n",
      " 'people to record store play pause and forward wind tv programmes when they '\n",
      " 'want. essentially the technology allows for much more personalised tv. they '\n",
      " 'are also being built-in to high-definition tv sets which are big business in '\n",
      " 'japan and the us but slower to take off in europe because of the lack of '\n",
      " 'high-definition programming. not only can people forward wind through '\n",
      " 'adverts they can also forget about abiding by network and channel schedules '\n",
      " 'putting together their own a-la-carte entertainment. but some us networks '\n",
      " 'and cable and satellite companies are worried about what it means for them '\n",
      " 'in terms of advertising revenues as well as brand identity and viewer '\n",
      " 'loyalty to channels. although the us leads in this technology at the moment '\n",
      " 'it is also a concern that is being raised in europe particularly with the '\n",
      " 'growing uptake of services like sky+. what happens here today we will see in '\n",
      " 'nine months to a years time in the uk adam hume the bbc broadcast s '\n",
      " 'futurologist told the bbc news website. for the likes of the bbc there are '\n",
      " 'no issues of lost advertising revenue yet. it is a more pressing issue at '\n",
      " 'the moment for commercial uk broadcasters but brand loyalty is important for '\n",
      " 'everyone. we will be talking more about content brands rather than network '\n",
      " 'brands said tim hanlon from brand communications firm starcom mediavest. the '\n",
      " 'reality is that with broadband connections anybody can be the producer of '\n",
      " 'content. he added: the challenge now is that it is hard to promote a '\n",
      " 'programme with so much choice. what this means said stacey jolna senior vice '\n",
      " 'president of tv guide tv group is that the way people find the content they '\n",
      " 'want to watch has to be simplified for tv viewers. it means that networks in '\n",
      " 'us terms or channels could take a leaf out of google s book and be the '\n",
      " 'search engine of the future instead of the scheduler to help people find '\n",
      " 'what they want to watch. this kind of channel model might work for the '\n",
      " 'younger ipod generation which is used to taking control of their gadgets and '\n",
      " 'what they play on them. but it might not suit everyone the panel recognised. '\n",
      " 'older generations are more comfortable with familiar schedules and channel '\n",
      " 'brands because they know what they are getting. they perhaps do not want so '\n",
      " 'much of the choice put into their hands mr hanlon suggested. on the other '\n",
      " 'end you have the kids just out of diapers who are pushing buttons already - '\n",
      " 'everything is possible and available to them said mr hanlon. ultimately the '\n",
      " 'consumer will tell the market they want. of the 50 000 new gadgets and '\n",
      " 'technologies being showcased at ces many of them are about enhancing the '\n",
      " 'tv-watching experience. high-definition tv sets are everywhere and many new '\n",
      " 'models of lcd (liquid crystal display) tvs have been launched with dvr '\n",
      " 'capability built into them instead of being external boxes. one such example '\n",
      " 'launched at the show is humax s 26-inch lcd tv with an 80-hour tivo dvr and '\n",
      " 'dvd recorder. one of the us s biggest satellite tv companies directtv has '\n",
      " 'even launched its own branded dvr at the show with 100-hours of recording '\n",
      " 'capability instant replay and a search function. the set can pause and '\n",
      " 'rewind tv for up to 90 hours. and microsoft chief bill gates announced in '\n",
      " 'his pre-show keynote speech a partnership with tivo called tivotogo which '\n",
      " 'means people can play recorded programmes on windows pcs and mobile devices. '\n",
      " 'all these reflect the increasing trend of freeing up multimedia so that '\n",
      " 'people can watch what they want when they want.')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Remove Emails\n",
    "data = [re.sub('\\S*@\\S*\\s?', '', sent) for sent in data]\n",
    "\n",
    "# Remove new line characters\n",
    "data = [re.sub('\\s+', ' ', sent) for sent in data]\n",
    "\n",
    "# Remove distracting single quotes\n",
    "data = [re.sub(\"\\'\",\" \", sent) for sent in data]\n",
    "\n",
    "pprint(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-24T12:43:13.634807Z",
     "start_time": "2022-04-24T12:43:11.528170Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tv', 'future', 'in', 'the', 'hands', 'of', 'viewers', 'with', 'home', 'theatre', 'systems', 'plasma', 'high', 'definition', 'tvs', 'and', 'digital', 'video', 'recorders', 'moving', 'into', 'the', 'living', 'room', 'the', 'way', 'people', 'watch', 'tv', 'will', 'be', 'radically', 'different', 'in', 'five', 'years', 'time', 'that', 'is', 'according', 'to', 'an', 'expert', 'panel', 'which', 'gathered', 'at', 'the', 'annual', 'consumer', 'electronics', 'show', 'in', 'las', 'vegas', 'to', 'discuss', 'how', 'these', 'new', 'technologies', 'will', 'impact', 'one', 'of', 'our', 'favourite', 'pastimes', 'with', 'the', 'us', 'leading', 'the', 'trend', 'programmes', 'and', 'other', 'content', 'will', 'be', 'delivered', 'to', 'viewers', 'via', 'home', 'networks', 'through', 'cable', 'satellite', 'telecoms', 'companies', 'and', 'broadband', 'service', 'providers', 'to', 'front', 'rooms', 'and', 'portable', 'devices', 'one', 'of', 'the', 'most', 'talked', 'about', 'technologies', 'of', 'ces', 'has', 'been', 'digital', 'and', 'personal', 'video', 'recorders', 'dvr', 'and', 'pvr', 'these', 'set', 'top', 'boxes', 'like', 'the', 'us', 'tivo', 'and', 'the', 'uk', 'sky', 'system', 'allow', 'people', 'to', 'record', 'store', 'play', 'pause', 'and', 'forward', 'wind', 'tv', 'programmes', 'when', 'they', 'want', 'essentially', 'the', 'technology', 'allows', 'for', 'much', 'more', 'personalised', 'tv', 'they', 'are', 'also', 'being', 'built', 'in', 'to', 'high', 'definition', 'tv', 'sets', 'which', 'are', 'big', 'business', 'in', 'japan', 'and', 'the', 'us', 'but', 'slower', 'to', 'take', 'off', 'in', 'europe', 'because', 'of', 'the', 'lack', 'of', 'high', 'definition', 'programming', 'not', 'only', 'can', 'people', 'forward', 'wind', 'through', 'adverts', 'they', 'can', 'also', 'forget', 'about', 'abiding', 'by', 'network', 'and', 'channel', 'schedules', 'putting', 'together', 'their', 'own', 'la', 'carte', 'entertainment', 'but', 'some', 'us', 'networks', 'and', 'cable', 'and', 'satellite', 'companies', 'are', 'worried', 'about', 'what', 'it', 'means', 'for', 'them', 'in', 'terms', 'of', 'advertising', 'revenues', 'as', 'well', 'as', 'brand', 'identity', 'and', 'viewer', 'loyalty', 'to', 'channels', 'although', 'the', 'us', 'leads', 'in', 'this', 'technology', 'at', 'the', 'moment', 'it', 'is', 'also', 'concern', 'that', 'is', 'being', 'raised', 'in', 'europe', 'particularly', 'with', 'the', 'growing', 'uptake', 'of', 'services', 'like', 'sky', 'what', 'happens', 'here', 'today', 'we', 'will', 'see', 'in', 'nine', 'months', 'to', 'years', 'time', 'in', 'the', 'uk', 'adam', 'hume', 'the', 'bbc', 'broadcast', 'futurologist', 'told', 'the', 'bbc', 'news', 'website', 'for', 'the', 'likes', 'of', 'the', 'bbc', 'there', 'are', 'no', 'issues', 'of', 'lost', 'advertising', 'revenue', 'yet', 'it', 'is', 'more', 'pressing', 'issue', 'at', 'the', 'moment', 'for', 'commercial', 'uk', 'broadcasters', 'but', 'brand', 'loyalty', 'is', 'important', 'for', 'everyone', 'we', 'will', 'be', 'talking', 'more', 'about', 'content', 'brands', 'rather', 'than', 'network', 'brands', 'said', 'tim', 'hanlon', 'from', 'brand', 'communications', 'firm', 'starcom', 'mediavest', 'the', 'reality', 'is', 'that', 'with', 'broadband', 'connections', 'anybody', 'can', 'be', 'the', 'producer', 'of', 'content', 'he', 'added', 'the', 'challenge', 'now', 'is', 'that', 'it', 'is', 'hard', 'to', 'promote', 'programme', 'with', 'so', 'much', 'choice', 'what', 'this', 'means', 'said', 'stacey', 'jolna', 'senior', 'vice', 'president', 'of', 'tv', 'guide', 'tv', 'group', 'is', 'that', 'the', 'way', 'people', 'find', 'the', 'content', 'they', 'want', 'to', 'watch', 'has', 'to', 'be', 'simplified', 'for', 'tv', 'viewers', 'it', 'means', 'that', 'networks', 'in', 'us', 'terms', 'or', 'channels', 'could', 'take', 'leaf', 'out', 'of', 'google', 'book', 'and', 'be', 'the', 'search', 'engine', 'of', 'the', 'future', 'instead', 'of', 'the', 'scheduler', 'to', 'help', 'people', 'find', 'what', 'they', 'want', 'to', 'watch', 'this', 'kind', 'of', 'channel', 'model', 'might', 'work', 'for', 'the', 'younger', 'ipod', 'generation', 'which', 'is', 'used', 'to', 'taking', 'control', 'of', 'their', 'gadgets', 'and', 'what', 'they', 'play', 'on', 'them', 'but', 'it', 'might', 'not', 'suit', 'everyone', 'the', 'panel', 'recognised', 'older', 'generations', 'are', 'more', 'comfortable', 'with', 'familiar', 'schedules', 'and', 'channel', 'brands', 'because', 'they', 'know', 'what', 'they', 'are', 'getting', 'they', 'perhaps', 'do', 'not', 'want', 'so', 'much', 'of', 'the', 'choice', 'put', 'into', 'their', 'hands', 'mr', 'hanlon', 'suggested', 'on', 'the', 'other', 'end', 'you', 'have', 'the', 'kids', 'just', 'out', 'of', 'diapers', 'who', 'are', 'pushing', 'buttons', 'already', 'everything', 'is', 'possible', 'and', 'available', 'to', 'them', 'said', 'mr', 'hanlon', 'ultimately', 'the', 'consumer', 'will', 'tell', 'the', 'market', 'they', 'want', 'of', 'the', 'new', 'gadgets', 'and', 'technologies', 'being', 'showcased', 'at', 'ces', 'many', 'of', 'them', 'are', 'about', 'enhancing', 'the', 'tv', 'watching', 'experience', 'high', 'definition', 'tv', 'sets', 'are', 'everywhere', 'and', 'many', 'new', 'models', 'of', 'lcd', 'liquid', 'crystal', 'display', 'tvs', 'have', 'been', 'launched', 'with', 'dvr', 'capability', 'built', 'into', 'them', 'instead', 'of', 'being', 'external', 'boxes', 'one', 'such', 'example', 'launched', 'at', 'the', 'show', 'is', 'humax', 'inch', 'lcd', 'tv', 'with', 'an', 'hour', 'tivo', 'dvr', 'and', 'dvd', 'recorder', 'one', 'of', 'the', 'us', 'biggest', 'satellite', 'tv', 'companies', 'directtv', 'has', 'even', 'launched', 'its', 'own', 'branded', 'dvr', 'at', 'the', 'show', 'with', 'hours', 'of', 'recording', 'capability', 'instant', 'replay', 'and', 'search', 'function', 'the', 'set', 'can', 'pause', 'and', 'rewind', 'tv', 'for', 'up', 'to', 'hours', 'and', 'microsoft', 'chief', 'bill', 'gates', 'announced', 'in', 'his', 'pre', 'show', 'keynote', 'speech', 'partnership', 'with', 'tivo', 'called', 'tivotogo', 'which', 'means', 'people', 'can', 'play', 'recorded', 'programmes', 'on', 'windows', 'pcs', 'and', 'mobile', 'devices', 'all', 'these', 'reflect', 'the', 'increasing', 'trend', 'of', 'freeing', 'up', 'multimedia', 'so', 'that', 'people', 'can', 'watch', 'what', 'they', 'want', 'when', 'they', 'want']\n"
     ]
    }
   ],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence).encode('utf-8'), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "data_words = list(sent_to_words(data))\n",
    "print(data_words[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-24T12:43:17.708874Z",
     "start_time": "2022-04-24T12:43:13.636770Z"
    }
   },
   "outputs": [],
   "source": [
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100)# higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-24T12:43:19.140882Z",
     "start_time": "2022-04-24T12:43:17.710873Z"
    }
   },
   "outputs": [],
   "source": [
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-24T12:43:19.155847Z",
     "start_time": "2022-04-24T12:43:19.142894Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tv', 'future', 'in', 'the', 'hands', 'of', 'viewers', 'with', 'home', 'theatre', 'systems', 'plasma', 'high_definition', 'tvs', 'and', 'digital', 'video_recorders', 'moving', 'into', 'the', 'living_room', 'the', 'way', 'people', 'watch', 'tv', 'will', 'be', 'radically', 'different', 'in', 'five', 'years', 'time', 'that', 'is', 'according', 'to', 'an', 'expert', 'panel', 'which', 'gathered', 'at', 'the', 'annual', 'consumer_electronics', 'show', 'in', 'las_vegas', 'to', 'discuss', 'how', 'these', 'new', 'technologies', 'will', 'impact', 'one', 'of', 'our', 'favourite', 'pastimes', 'with', 'the', 'us', 'leading', 'the', 'trend', 'programmes', 'and', 'other', 'content', 'will', 'be', 'delivered', 'to', 'viewers', 'via', 'home', 'networks', 'through', 'cable', 'satellite', 'telecoms', 'companies', 'and', 'broadband', 'service_providers', 'to', 'front', 'rooms', 'and', 'portable_devices', 'one', 'of', 'the', 'most', 'talked', 'about', 'technologies', 'of', 'ces', 'has', 'been', 'digital', 'and', 'personal', 'video_recorders', 'dvr', 'and', 'pvr', 'these', 'set', 'top_boxes', 'like', 'the', 'us', 'tivo', 'and', 'the', 'uk', 'sky', 'system', 'allow', 'people', 'to', 'record', 'store', 'play', 'pause', 'and', 'forward', 'wind', 'tv_programmes', 'when', 'they', 'want', 'essentially', 'the', 'technology', 'allows', 'for', 'much', 'more', 'personalised', 'tv', 'they', 'are', 'also', 'being', 'built', 'in', 'to', 'high_definition', 'tv', 'sets', 'which', 'are', 'big', 'business', 'in', 'japan', 'and', 'the', 'us', 'but', 'slower', 'to', 'take', 'off', 'in', 'europe', 'because', 'of', 'the', 'lack', 'of', 'high_definition', 'programming', 'not', 'only', 'can', 'people', 'forward', 'wind', 'through', 'adverts', 'they', 'can', 'also', 'forget', 'about', 'abiding', 'by', 'network', 'and', 'channel', 'schedules', 'putting', 'together', 'their', 'own', 'la', 'carte', 'entertainment', 'but', 'some', 'us', 'networks', 'and', 'cable', 'and', 'satellite', 'companies', 'are', 'worried', 'about', 'what', 'it', 'means', 'for', 'them', 'in', 'terms', 'of', 'advertising', 'revenues', 'as', 'well', 'as', 'brand', 'identity', 'and', 'viewer', 'loyalty', 'to', 'channels', 'although', 'the', 'us', 'leads', 'in', 'this', 'technology', 'at', 'the', 'moment', 'it', 'is', 'also', 'concern', 'that', 'is', 'being', 'raised', 'in', 'europe', 'particularly', 'with', 'the', 'growing', 'uptake', 'of', 'services', 'like', 'sky', 'what', 'happens', 'here', 'today', 'we', 'will', 'see', 'in', 'nine', 'months', 'to', 'years', 'time', 'in', 'the', 'uk', 'adam', 'hume', 'the', 'bbc', 'broadcast', 'futurologist', 'told', 'the', 'bbc_news', 'website', 'for', 'the', 'likes', 'of', 'the', 'bbc', 'there', 'are', 'no', 'issues', 'of', 'lost', 'advertising', 'revenue', 'yet', 'it', 'is', 'more', 'pressing', 'issue', 'at', 'the', 'moment', 'for', 'commercial', 'uk', 'broadcasters', 'but', 'brand', 'loyalty', 'is', 'important', 'for', 'everyone', 'we', 'will', 'be', 'talking', 'more', 'about', 'content', 'brands', 'rather_than', 'network', 'brands', 'said', 'tim', 'hanlon', 'from', 'brand', 'communications', 'firm', 'starcom', 'mediavest', 'the', 'reality', 'is', 'that', 'with', 'broadband', 'connections', 'anybody', 'can', 'be', 'the', 'producer', 'of', 'content', 'he', 'added', 'the', 'challenge', 'now', 'is', 'that', 'it', 'is', 'hard', 'to', 'promote', 'programme', 'with', 'so', 'much', 'choice', 'what', 'this', 'means', 'said', 'stacey', 'jolna', 'senior_vice', 'president', 'of', 'tv', 'guide', 'tv', 'group', 'is', 'that', 'the', 'way', 'people', 'find', 'the', 'content', 'they', 'want', 'to', 'watch', 'has', 'to', 'be', 'simplified', 'for', 'tv', 'viewers', 'it', 'means', 'that', 'networks', 'in', 'us', 'terms', 'or', 'channels', 'could', 'take', 'leaf', 'out', 'of', 'google', 'book', 'and', 'be', 'the', 'search_engine', 'of', 'the', 'future', 'instead', 'of', 'the', 'scheduler', 'to', 'help', 'people', 'find', 'what', 'they', 'want', 'to', 'watch', 'this', 'kind', 'of', 'channel', 'model', 'might', 'work', 'for', 'the', 'younger', 'ipod', 'generation', 'which', 'is', 'used', 'to', 'taking', 'control', 'of', 'their', 'gadgets', 'and', 'what', 'they', 'play', 'on', 'them', 'but', 'it', 'might', 'not', 'suit', 'everyone', 'the', 'panel', 'recognised', 'older', 'generations', 'are', 'more', 'comfortable', 'with', 'familiar', 'schedules', 'and', 'channel', 'brands', 'because', 'they', 'know', 'what', 'they', 'are', 'getting', 'they', 'perhaps', 'do', 'not', 'want', 'so', 'much', 'of', 'the', 'choice', 'put', 'into', 'their', 'hands', 'mr', 'hanlon', 'suggested', 'on', 'the', 'other', 'end', 'you', 'have', 'the', 'kids', 'just', 'out', 'of', 'diapers', 'who', 'are', 'pushing', 'buttons', 'already', 'everything', 'is', 'possible', 'and', 'available', 'to', 'them', 'said', 'mr', 'hanlon', 'ultimately', 'the', 'consumer', 'will', 'tell', 'the', 'market', 'they', 'want', 'of', 'the', 'new', 'gadgets', 'and', 'technologies', 'being', 'showcased', 'at', 'ces', 'many', 'of', 'them', 'are', 'about', 'enhancing', 'the', 'tv', 'watching', 'experience', 'high_definition', 'tv', 'sets', 'are', 'everywhere', 'and', 'many', 'new', 'models', 'of', 'lcd', 'liquid', 'crystal', 'display', 'tvs', 'have', 'been', 'launched', 'with', 'dvr', 'capability', 'built', 'into', 'them', 'instead', 'of', 'being', 'external', 'boxes', 'one', 'such', 'example', 'launched', 'at', 'the', 'show', 'is', 'humax', 'inch', 'lcd', 'tv', 'with', 'an', 'hour', 'tivo', 'dvr', 'and', 'dvd', 'recorder', 'one', 'of', 'the', 'us', 'biggest', 'satellite', 'tv', 'companies', 'directtv', 'has', 'even', 'launched', 'its', 'own', 'branded', 'dvr', 'at', 'the', 'show', 'with', 'hours', 'of', 'recording', 'capability', 'instant', 'replay', 'and', 'search', 'function', 'the', 'set', 'can', 'pause', 'and', 'rewind', 'tv', 'for', 'up', 'to', 'hours', 'and', 'microsoft', 'chief', 'bill_gates', 'announced', 'in', 'his', 'pre', 'show', 'keynote_speech', 'partnership', 'with', 'tivo', 'called', 'tivotogo', 'which', 'means', 'people', 'can', 'play', 'recorded', 'programmes', 'on', 'windows', 'pcs', 'and', 'mobile', 'devices', 'all', 'these', 'reflect', 'the', 'increasing', 'trend', 'of', 'freeing', 'up', 'multimedia', 'so', 'that', 'people', 'can', 'watch', 'what', 'they', 'want', 'when', 'they', 'want']\n"
     ]
    }
   ],
   "source": [
    "# See bigram example\n",
    "print(bigram_mod[data_words[0]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-24T12:43:19.170895Z",
     "start_time": "2022-04-24T12:43:19.158840Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-04-24T12:42:57.540Z"
    }
   },
   "outputs": [],
   "source": [
    "# Remove Stop Words\n",
    "data_words_nostops = remove_stopwords(data_words)\n",
    "\n",
    "# Form Bigrams\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-04-24T12:42:57.542Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(len(data_lemmatized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-04-24T12:42:57.544Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "# Create Corpus\n",
    "texts = data_lemmatized\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "# View\n",
    "print(corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-04-24T12:42:57.547Z"
    }
   },
   "outputs": [],
   "source": [
    "id2word[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-04-24T12:42:57.549Z"
    }
   },
   "outputs": [],
   "source": [
    "# Human readable format of corpus (term-frequency)\n",
    "[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-04-24T12:42:57.550Z"
    }
   },
   "outputs": [],
   "source": [
    "def tune_model(dictionary, corpus, texts, limit, start=2, step=1):\n",
    "    \n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in tqdm(range(start, limit, step)):\n",
    "        model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=num_topics, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(round(coherencemodel.get_coherence(),3))\n",
    "\n",
    "    return model_list, coherence_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-04-24T12:42:57.553Z"
    },
    "scrolled": true
   },
   "source": [
    "## Can take a long time to run\n",
    "model_list, coherence_values = tune_model(dictionary=id2word, corpus=corpus, texts=data_lemmatized, start=2, limit=8, step=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show graph\n",
    "limit=8; start=2; step=1;\n",
    "x = range(start, limit, step)\n",
    "plt.plot(x, coherence_values)\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.legend((\"coherence_values\"), loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the coherence scores\n",
    "for m, cv in zip(x, coherence_values):\n",
    "    print(\"Num Topics =\", m, \" has Coherence Value of\", round(cv, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_model = model_list[3]\n",
    "model_topics = optimal_model.show_topics(formatted=False)\n",
    "pprint(optimal_model.print_topics(num_words=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = optimal_model.show_topics(num_topics=5, num_words=10, formatted=False)\n",
    "topics_words = [(tp[0], [wd[0] for wd in tp[1]]) for tp in x]\n",
    "\n",
    "# Below Code Prints Topics and Words\n",
    "for topic, words in topics_words:\n",
    "    print(str(topic) + \"::\" + str(words))\n",
    "print()\n",
    "\n",
    "print(\"---------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from functools import reduce\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _document_topic_matrix(model, doc_term_matrix):\n",
    "\n",
    "    a = reduce(operator.concat, model[doc_term_matrix])\n",
    "    d = defaultdict(list)\n",
    "    for tup in a:\n",
    "        d[tup[0]] += (tup[1],)\n",
    "    df = pd.DataFrame.from_dict(d)\n",
    "    doc_topic_matrix = df.values\n",
    "    return doc_topic_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = _document_topic_matrix(optimal_model, corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get main topic in each document\n",
    "for i, row in enumerate(optimal_model[corpus]):\n",
    "    \n",
    "    sent_topics_df = pd.DataFrame()\n",
    "    \n",
    "    print(row[0])\n",
    "    row = sorted(row[0], key=lambda x: x[1], reverse=True)\n",
    "    print(row)\n",
    "    \n",
    "    for j, (topic_num, prop_topic) in enumerate(row):\n",
    "        print(j, \"topic: \", topic_num, \"prob_topic\", prob_topic)\n",
    "        \n",
    "        if j == 0:  # => dominant topic\n",
    "            wp = optimal_model.show_topic(topic_num)\n",
    "            print(\"wp: \", wp)\n",
    "            topic_keywords = \", \".join([word for word, prop in wp])\n",
    "            print(topic_keywords)\n",
    "            sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_topics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_topics_sentences(ldamodel=optimal_model, corpus=corpus, texts=data):\n",
    "    # Init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "\n",
    "    # Get main topic in each document\n",
    "    for i, row in enumerate(ldamodel[corpus]):\n",
    "        \n",
    "        row = sorted(row[0], key=lambda x: x[1], reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "    # Add original text to the end of the output\n",
    "    contents = pd.Series(texts)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    return(sent_topics_df)\n",
    "\n",
    "\n",
    "df_topic_sents_keywords = format_topics_sentences(ldamodel=optimal_model, corpus=corpus, texts=data)\n",
    "\n",
    "# Format\n",
    "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "df_dominant_topic.columns = ['Document_No', 'Topic_No', 'Topic_Perc_Contrib', 'Topic_Keywords', 'Text']\n",
    "\n",
    "final_df = df_dominant_topic.drop(\"Document_No\", axis=1)\n",
    "\n",
    "# add clean text Show\n",
    "clean_text = [ \" \".join(i) for i in texts]\n",
    "final_df[\"Clean_Text\"] = clean_text\n",
    "\n",
    "# show\n",
    "final_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df[\"Topic_No\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec  \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer   \n",
    "import pickle\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification using TFIDF vectorizer\n",
    "\n",
    "# Vectorize training and testing data. Here we would pass TfidfVectorizer() to vec \n",
    "def Vectorize(vec, X_train, X_test):    \n",
    "    \n",
    "    X_train_vec = vec.fit_transform(X_train)\n",
    "    X_test_vec = vec.transform(X_test)\n",
    "    \n",
    "    print('Vectorization complete.\\n')\n",
    "    \n",
    "    \n",
    "    return X_train_vec, X_test_vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use multiple classifiers and grid search for prediction\n",
    "def ML_modeling(models, params, X_train, X_test, y_train, y_test):    \n",
    "    \n",
    "    if not set(models.keys()).issubset(set(params.keys())):\n",
    "        raise ValueError('Some estimators are missing parameters')\n",
    "\n",
    "    for key in models.keys():\n",
    "    \n",
    "        model = models[key]\n",
    "        param = params[key]\n",
    "        gs = GridSearchCV(model, param, cv=5, error_score=0, refit=True)\n",
    "        gs.fit(X_train, y_train)\n",
    "        y_pred = gs.predict(X_test)\n",
    "        \n",
    "        # Print scores for the classifier\n",
    "        print(key, ':', gs.best_params_)\n",
    "        print(\"Accuracy: %1.3f \\tPrecision: %1.3f \\tRecall: %1.3f \\t\\tF1: %1.3f\\n\" % (accuracy_score(y_test, y_pred), precision_score(y_test, y_pred, average='macro'), recall_score(y_test, y_pred, average='macro'), f1_score(y_test, y_pred, average='macro')))\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preparing to make a pipeline \n",
    "models = {\n",
    "    'Naive Bayes': MultinomialNB(), \n",
    "    'Decision Tree': DecisionTreeClassifier(),  \n",
    "#     'Perceptron': MLPClassifier(),\n",
    "    'Gradient Boosting': GradientBoostingClassifier()   ## This model would take a little longer to run \n",
    "}\n",
    "\n",
    "params = {\n",
    "    'Naive Bayes': { 'alpha': [0.5, 1], 'fit_prior': [True, False] }, \n",
    "    'Decision Tree': { 'min_samples_split': [1, 2, 5] }, \n",
    "#     'Perceptron': { 'alpha': [0.0001, 0.001], 'activation': ['tanh', 'relu'] },\n",
    "    'Gradient Boosting': { 'learning_rate': [0.05, 0.1], 'min_samples_split': [2, 5] }\n",
    "}\n",
    "\n",
    "\n",
    "# Encode label categories to numbers\n",
    "enc = LabelEncoder()\n",
    "final_df['Topic_No'] = enc.fit_transform(final_df['Topic_No'])\n",
    "labels = list(enc.classes_)\n",
    "\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split and vectorize\n",
    "X_train, X_test, y_train, y_test = train_test_split(final_df['Text'], final_df['Topic_No'], test_size=0.2, shuffle=True)\n",
    "X_train_vec, X_test_vec = Vectorize(TfidfVectorizer(), X_train, X_test)\n",
    "\n",
    "ML_modeling(models, params, X_train_vec, X_test_vec, y_train, y_test)\n",
    "## ML_modeling method also prints performance scores for each classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('filename.pickle', 'rb') as handle:\n",
    "    b = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split and vectorize\n",
    "X_train, X_test, y_train, y_test = train_test_split(final_df['Text'], final_df['Topic_No'], test_size=0.2, shuffle=True)\n",
    "X_train_vec, X_test_vec = Vectorize(TfidfVectorizer(), X_train, X_test)\n",
    "\n",
    "ML_modeling(models, params, X_train_vec, X_test_vec, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize training and testing data. Here we would pass TfidfVectorizer() to vec \n",
    "def Vectorize(vec, X_train, X_test):    \n",
    "    \n",
    "    X_train_vec = vec.fit_transform(X_train)\n",
    "    X_test_vec = vec.transform(X_test)\n",
    "    \n",
    "    with open('Vectorizer.pickle', 'wb') as handle:\n",
    "        pickle.dump(vec, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    print('Vectorization complete.\\n')\n",
    "    \n",
    "    \n",
    "    return X_train_vec, X_test_vec\n",
    "\n",
    "def build_model(models, params, X_train, X_test, y_train, y_test):\n",
    "    if not set(models.keys()).issubset(set(params.keys())):\n",
    "        raise ValueError('Some estimators are missing parameters')\n",
    "\n",
    "    for key in models.keys():\n",
    "    \n",
    "        model = models[key]\n",
    "        param = params[key]\n",
    "        gs = GridSearchCV(model, param, cv=5, error_score=0, refit=True)\n",
    "        gs.fit(X_train, y_train)\n",
    "        y_pred = gs.predict(X_test)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Print scores for the classifier\n",
    "        print(key, ':', gs.best_params_)\n",
    "        print(\"Accuracy: %1.3f \\tPrecision: %1.3f \\tRecall: %1.3f \\t\\tF1: %1.3f\\n\" % (accuracy_score(y_test, y_pred), precision_score(y_test, y_pred, average='macro'), recall_score(y_test, y_pred, average='macro'), f1_score(y_test, y_pred, average='macro')))\n",
    "    \n",
    "    # dump model\n",
    "    with open('model.pickle', 'wb') as handle:\n",
    "        pickle.dump(gs, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    print(\"Model saved..\")\n",
    "    \n",
    "    return gs\n",
    "\n",
    "def clean_input(data):\n",
    "#     data = re.sub(r\"\\b\" + key + r\"\\b\", value, data.lower())\n",
    "    data = re.sub(\"\\s+\", \" \", str(data))\n",
    "    data = re.sub(\"\\S*@\\S*\\s?\", \"\", str(data))\n",
    "    data = re.sub(r\"https?://\\S+|www\\.\\S+\", \"\", str(data))\n",
    "    data = re.sub(\"(\\\\d|\\\\W)+\", \" \", str(data))\n",
    "    data = re.sub(\"'\", \"\", data)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split and vectorize\n",
    "X_train, X_test, y_train, y_test = train_test_split(final_df['Clean_Text'], final_df['Topic_No'], test_size=0.2, shuffle=True)\n",
    "X_train_vec, X_test_vec = Vectorize(TfidfVectorizer(), X_train, X_test)\n",
    "\n",
    "models = {\n",
    "    'Naive Bayes': MultinomialNB()\n",
    "}\n",
    "\n",
    "params = {\n",
    "    'Naive Bayes': { 'alpha': [0.5, 1], 'fit_prior': [True, False] } \n",
    "}\n",
    "\n",
    "model = build_model(models, params, X_train_vec, X_test_vec, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"\"\"\n",
    "Computer technology is leading to more accurate sizing and rating methods for process equipment. Heat exchangers are designed with high-precision prediction methods and complex numerical techniques to account for the local flow and temperature conditions. Fouling mitigation is just changing from an art to a science-based technology. The real benefits of sophisticated design codes will not be achieved without reliable fouling prediction methods and mitigation techniques that can be incorporated into the design phase. Recent developments in the computer technology provide an opportunity to productively use fouling information that is scattered in the literature, industry log books, and in reports. A long-term goal for the industry is to develop a knowledge-based system for designing and operating heat exchangers with a minimum impact of water fouling. The major challenges to develop the knowledge-based system are as follows:\n",
    "Compilation and organization of the fouling data\n",
    "Easy access by both research organizations and industry\n",
    "Development of a logic system for interpreting the fouling data\n",
    "Industrial acceptance of such knowledge-based systems\n",
    "The fouling data and analyses reported in the literature are often inadequately utilized. To effectively utilize these data, compilation and organization of fouling information are major tasks and challenges for research organizations and industries. This task is more important for the fouling-mitigation technology as compared to other engineering areas. It is hard to imagine that there could be a single correlation with a given set of parameters that can be used to predict the rate of fouling or determine the threshold fouling conditions. One has to rely on the best possible approach to mitigate fouling for a given set of conditions. In the absence of such information, either mitigation methods are overutilized, with high chemical costs, or underutilized, with high maintenance and production costs. Perrakis et al. [54] introduced an approach for compiling the fouling data in the literature. A major cooperative effort is required to carry out such a major challenge for compiling and organizing data. The fouling data and analyses must be accompanied with physical and chemical parameters without which the data have little value.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model file\n",
    "with open('model.pickle', 'rb') as handle:\n",
    "    NB_model = pickle.load(handle)\n",
    "    \n",
    "# load vectorizer file\n",
    "with open('Vectorizer.pickle', 'rb') as handle:\n",
    "    vect = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat =  vect.transform([clean_input(input_text)]).toarray()\n",
    "pred = NB_model.predict(feat)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
